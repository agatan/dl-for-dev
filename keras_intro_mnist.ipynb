{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XOlneX2MVXDE"
   },
   "source": [
    "## Keras: introduction\n",
    "\n",
    "keras は deep learning を簡単に記述することができるフレームワークです。\n",
    "高レベル API を提供するための wrapper ライブラリというような位置づけで、バックエンド（数値計算部分）を tensorflow にしたり theano にしたりできます。\n",
    "さらに最近では tensorflow に取り込まれ、`tf.keras` という形で利用することもできるようになっています。\n",
    "\n",
    "`tf.keras` を使うことで、 tensorflow のエコシステムに乗っかりつつ、簡単な記述でニューラルネットワークを学習させることができます。\n",
    "というわけで今回は `tf.keras` を使ってみます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MGuZqcF0Wc6m"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vqU9aCyod6GS"
   },
   "source": [
    "###  deep learning 界の Hello, world\n",
    "\n",
    "deep learning の世界では、Hello, world として MNIST がよく使われます。\n",
    "MNIST は 0~9 まで手書き文字を認識するというタスクで、データサイズも小さく問題も簡単です。\n",
    "\n",
    "有名なフレームワークは大体 MNIST データセットをダウンロードする utility 関数を提供しています。keras にもあるのでそれを使ってデータをダウンロードし、何枚かデータを見てみます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 449
    },
    "colab_type": "code",
    "id": "-ITBm_rImZG4",
    "outputId": "35e56987-fd9c-4d2a-bb62-8035a57d6fc3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type of images:  <class 'numpy.ndarray'>\n",
      "shape of images:  (60000, 28, 28)\n",
      "type of labels:  <class 'numpy.ndarray'>\n",
      "shape of labels:  (60000,)\n",
      "label of images[0] is  5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fd1bc9cc320>"
      ]
     },
     "execution_count": 39,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUsAAAFKCAYAAACU6307AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fd1bdf46fd0>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "(images, labels), (test_images, test_labels) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "print(\"type of images: \", type(images))\n",
    "print(\"shape of images: \", images.shape)\n",
    "print(\"type of labels: \", type(labels))\n",
    "print(\"shape of labels: \", labels.shape)\n",
    "\n",
    "\n",
    "print(\"label of images[0] is \", labels[0])\n",
    "plt.imshow(images[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MvEayIuxnYSP"
   },
   "source": [
    "全部で 60000 枚の画像と正解ラベルが与えられています。\n",
    "それぞれの画像は、サイズ 28x28 、grayscale になっていることがわかります。\n",
    "そして正解ラベルは 0~9 までの数字が入っていることもわかります。\n",
    "\n",
    "通常画像データは RGB など複数のチャネルを持っているので、WxHxC の 3 次元行列として扱います。\n",
    "今回のデータは 1 チャネルしかないのですが、形式を揃えるために 28x28x1 に reshape しておきます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wYxhJzziojcz"
   },
   "outputs": [],
   "source": [
    "images = np.expand_dims(images, axis=-1)\n",
    "test_images = np.expand_dims(test_images, axis=-1)\n",
    "\n",
    "labels = tf.keras.utils.to_categorical(labels, 10)\n",
    "test_labels = tf.keras.utils.to_categorical(test_labels, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "g5iUt2QUu-E8"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "owNrSgasu-Zr"
   },
   "source": [
    "画像を入力に、0~9 までの文字である確率をそれぞれ返すようなモデルを作ることを目標にします。\n",
    "\n",
    "例えば↑の画像を入力すると、`0 である確率: 0.01, 1 である確率: 0.01, 2 である確率: 0.04, ..., 5 である確率: 0.88, ..., 9 である確率: 0.02` のような出力が得られるというイメージです。  \n",
    "なので、この場合は `28x28x1` の行列から `10` 要素のベクトルを得られるようなモデルを書くことが必要になります。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WBhwG-sfn35n"
   },
   "source": [
    "## Keras のモデル例\n",
    "\n",
    "いきなりですが、これらの画像からラベルを当てるようなモデル例を keras で記述してみます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 527
    },
    "colab_type": "code",
    "id": "9ZZHMGc2msst",
    "outputId": "38fe342a-860c-4841-e311-debf834fdd8a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_13 (InputLayer)        (None, 28, 28, 1)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_24 (Conv2D)           (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "activation_48 (Activation)   (None, 26, 26, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_25 (Conv2D)           (None, 24, 24, 64)        18496     \n",
      "_________________________________________________________________\n",
      "activation_49 (Activation)   (None, 24, 24, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_12 (MaxPooling (None, 12, 12, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten_12 (Flatten)         (None, 9216)              0         \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 128)               1179776   \n",
      "_________________________________________________________________\n",
      "activation_50 (Activation)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_25 (Dense)             (None, 10)                1290      \n",
      "_________________________________________________________________\n",
      "activation_51 (Activation)   (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 1,199,882\n",
      "Trainable params: 1,199,882\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.engine.training.Model at 0x7fd1b8fa2c18>"
      ]
     },
     "execution_count": 41,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.python.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense, Activation\n",
    "\n",
    "\n",
    "def create_model():\n",
    "    input = Input(shape=(28, 28, 1))\n",
    "    x = Conv2D(32, kernel_size=3)(input)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Conv2D(64, kernel_size=3)(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = MaxPooling2D(pool_size=2)(x)\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(128)(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Dense(10)(x)\n",
    "    output = Activation('softmax')(x)\n",
    "\n",
    "    return tf.keras.Model(input, output)\n",
    "\n",
    "model = create_model()\n",
    "model.summary()\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LGwW4Uj0qeYN"
   },
   "source": [
    "これが keras の典型的なモデルの定義方法です。\n",
    "\n",
    "一行ずつ実行しながらコメントつけてみます。\n",
    "行列の shape に注目してみるとどんなことをしているのかなんとなく把握できるかもしれません。\n",
    "\n",
    "フレームワークを使った deep learning は、行列の shape をあわせる作業です。\n",
    "入力行列の shape が、層を経るごとにどう変化していくかを把握できると、自分で NN を組み立てるヒントになります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "mayp_IrYpkXl",
    "outputId": "4b00d7ea-e7d1-4e27-c3eb-16e5ae72b43c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'input_14:0' shape=(?, 28, 28, 1) dtype=float32>"
      ]
     },
     "execution_count": 42,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.python.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense, Activation\n",
    "\n",
    "# まず入力となる（計算グラフ上の）変数を定義します。後でこの変数に実際の画像データを入力し、計算グラフを評価することになります。\n",
    "input = Input(shape=(28, 28, 1))\n",
    "input # shape は (?, 28, 28, 1) になります。? はバッチサイズというもので、計算実行時に勝手に埋められるので今は無視して構いません。（学習時には複数枚同時に学習します。一度に見る枚数をバッチサイズといいます。たとえば 32 枚同時に学習する場合は、(32, 28, 28, 1) になります。）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "veKqwUP-sjAq",
    "outputId": "c1fe888a-bc5e-4af5-bafc-5e090e48752a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'conv2d_26/BiasAdd:0' shape=(?, 28, 28, 32) dtype=float32>"
      ]
     },
     "execution_count": 43,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 二次元の convolution を行います。(今回は詳細は省きますが) f(x) = w * x + b のちょっと特殊な形だと考えてください。\n",
    "x = Conv2D(32, kernel_size=3, padding='same')(input)\n",
    "x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "CCETl_2KtIR5",
    "outputId": "a6eaed4a-82da-41e9-feee-49b44697f195"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'activation_52/Relu:0' shape=(?, 28, 28, 32) dtype=float32>"
      ]
     },
     "execution_count": 44,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convolution の結果を活性化関数に入れます。活性化関数として今回は relu というものを使いました。（有名な活性化関数で、最近の NN では relu を使うのが主流だと思います。）\n",
    "x = Activation('relu')(x)\n",
    "x # 活性化関数は値だけを変化させるので shape は変わりません。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "iisqbaPbtLq1",
    "outputId": "cddbd155-67d2-4fbb-fb20-3b972e3154fa"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'activation_53/Relu:0' shape=(?, 28, 28, 64) dtype=float32>"
      ]
     },
     "execution_count": 45,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ↑と同じものを繰り返します。\n",
    "x = Conv2D(64, kernel_size=3, padding='same')(x)\n",
    "x = Activation('relu')(x)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "ActQwCNvtTOj",
    "outputId": "5639dadb-e3ac-4ef2-b65b-3f550bb47818"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'max_pooling2d_13/MaxPool:0' shape=(?, 14, 14, 64) dtype=float32>"
      ]
     },
     "execution_count": 46,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# MaxPooling はちょっと特殊な補助層のようなものです。2x2 のマスごとに最大値をとる操作をします。\n",
    "x = MaxPooling2D(pool_size=2)(x)\n",
    "x # 2x2 → 1x1 になるので、縦横ともに半分になります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "scuNyunhtdmQ",
    "outputId": "ce38a9ce-1903-4fd4-f07b-fee42ef87ad7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'flatten_13/Reshape:0' shape=(?, 12544) dtype=float32>"
      ]
     },
     "execution_count": 47,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 行列を平らにします。14x14x64 の行列を 12,544 の行列（ベクトル）に変換するだけです。値は変わりません。\n",
    "x = Flatten()(x)\n",
    "x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Wt2vaZrit3Z7",
    "outputId": "2893d58e-ddc1-48c1-bdf3-803733c520c9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'activation_54/Relu:0' shape=(?, 128) dtype=float32>"
      ]
     },
     "execution_count": 48,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dense は w * x + b そのものを行う層です。w は 12544x128 の行列、x は↑なので 12544 の行列です。つまり w * x の shape は 128 になります。\n",
    "x = Dense(128)(x)\n",
    "x = Activation('relu')(x)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "JXoB3qbGuMTq",
    "outputId": "d405b9a7-a0dd-4dd3-ce94-9fbc19db5d55"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'activation_55/Softmax:0' shape=(?, 10) dtype=float32>"
      ]
     },
     "execution_count": 49,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ↑と同じく w * x + b を行います。ここでの w は 128x10 の行列になります。\n",
    "x = Dense(10)(x)\n",
    "output = Activation('softmax')(x)\n",
    "output # 今回の問題は 10 クラス分類なので、10 次元のベクトルを返すようにします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "tzpwAziQuqIz",
    "outputId": "52163d06-13ba-404b-d19a-7f99a19554ce"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.engine.training.Model at 0x7fd1b8edb668>"
      ]
     },
     "execution_count": 50,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "model = tf.keras.Model(input, output)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VDhi9hqt58QR"
   },
   "source": [
    "## 学習\n",
    "\n",
    "学習は以下のように行います"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 3451
    },
    "colab_type": "code",
    "id": "gpIB4L3aWKJs",
    "outputId": "5f278d79-86f8-4951-f160-c5899df9ed30"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/100\n",
      "60000/60000 [==============================] - 11s 175us/step - loss: 11.8661 - acc: 0.2540 - val_loss: 0.2185 - val_acc: 0.9378\n",
      "Epoch 2/100\n",
      "60000/60000 [==============================] - 10s 167us/step - loss: 0.0976 - acc: 0.9706 - val_loss: 0.0686 - val_acc: 0.9792\n",
      "Epoch 3/100\n",
      "60000/60000 [==============================] - 10s 166us/step - loss: 0.0397 - acc: 0.9879 - val_loss: 0.0425 - val_acc: 0.9864\n",
      "Epoch 4/100\n",
      "60000/60000 [==============================] - 10s 168us/step - loss: 0.0228 - acc: 0.9932 - val_loss: 0.0406 - val_acc: 0.9878\n",
      "Epoch 5/100\n",
      "60000/60000 [==============================] - 10s 165us/step - loss: 0.0143 - acc: 0.9958 - val_loss: 0.0466 - val_acc: 0.9863\n",
      "Epoch 6/100\n",
      "60000/60000 [==============================] - 10s 167us/step - loss: 0.0085 - acc: 0.9976 - val_loss: 0.0419 - val_acc: 0.9887\n",
      "Epoch 7/100\n",
      "60000/60000 [==============================] - 10s 165us/step - loss: 0.0044 - acc: 0.9989 - val_loss: 0.0473 - val_acc: 0.9878\n",
      "Epoch 8/100\n",
      "60000/60000 [==============================] - 10s 167us/step - loss: 0.0022 - acc: 0.9996 - val_loss: 0.0430 - val_acc: 0.9904\n",
      "Epoch 9/100\n",
      "60000/60000 [==============================] - 10s 165us/step - loss: 0.0015 - acc: 0.9998 - val_loss: 0.0443 - val_acc: 0.9897\n",
      "Epoch 10/100\n",
      "60000/60000 [==============================] - 10s 168us/step - loss: 7.4597e-04 - acc: 0.9999 - val_loss: 0.0448 - val_acc: 0.9904\n",
      "Epoch 11/100\n",
      "60000/60000 [==============================] - 10s 165us/step - loss: 5.0589e-04 - acc: 1.0000 - val_loss: 0.0445 - val_acc: 0.9906\n",
      "Epoch 12/100\n",
      "60000/60000 [==============================] - 10s 168us/step - loss: 4.2620e-04 - acc: 1.0000 - val_loss: 0.0459 - val_acc: 0.9906\n",
      "Epoch 13/100\n",
      "60000/60000 [==============================] - 10s 166us/step - loss: 3.9672e-04 - acc: 1.0000 - val_loss: 0.0466 - val_acc: 0.9903\n",
      "Epoch 14/100\n",
      "60000/60000 [==============================] - 10s 167us/step - loss: 3.7775e-04 - acc: 1.0000 - val_loss: 0.0469 - val_acc: 0.9903\n",
      "Epoch 15/100\n",
      "60000/60000 [==============================] - 10s 165us/step - loss: 3.6303e-04 - acc: 1.0000 - val_loss: 0.0475 - val_acc: 0.9901\n",
      "Epoch 16/100\n",
      "60000/60000 [==============================] - 10s 168us/step - loss: 3.5250e-04 - acc: 1.0000 - val_loss: 0.0479 - val_acc: 0.9900\n",
      "Epoch 17/100\n",
      "60000/60000 [==============================] - 10s 166us/step - loss: 3.4367e-04 - acc: 1.0000 - val_loss: 0.0478 - val_acc: 0.9901\n",
      "Epoch 18/100\n",
      "60000/60000 [==============================] - 10s 168us/step - loss: 3.3755e-04 - acc: 1.0000 - val_loss: 0.0488 - val_acc: 0.9900\n",
      "Epoch 19/100\n",
      "60000/60000 [==============================] - 10s 167us/step - loss: 3.3178e-04 - acc: 1.0000 - val_loss: 0.0491 - val_acc: 0.9899\n",
      "Epoch 20/100\n",
      "60000/60000 [==============================] - 10s 168us/step - loss: 3.2697e-04 - acc: 1.0000 - val_loss: 0.0496 - val_acc: 0.9898\n",
      "Epoch 21/100\n",
      "60000/60000 [==============================] - 10s 166us/step - loss: 3.2304e-04 - acc: 1.0000 - val_loss: 0.0499 - val_acc: 0.9902\n",
      "Epoch 22/100\n",
      "60000/60000 [==============================] - 10s 168us/step - loss: 3.1934e-04 - acc: 1.0000 - val_loss: 0.0501 - val_acc: 0.9899\n",
      "Epoch 23/100\n",
      "60000/60000 [==============================] - 10s 166us/step - loss: 3.1604e-04 - acc: 1.0000 - val_loss: 0.0505 - val_acc: 0.9901\n",
      "Epoch 24/100\n",
      "60000/60000 [==============================] - 10s 169us/step - loss: 3.1341e-04 - acc: 1.0000 - val_loss: 0.0509 - val_acc: 0.9897\n",
      "Epoch 25/100\n",
      "60000/60000 [==============================] - 10s 166us/step - loss: 3.1091e-04 - acc: 1.0000 - val_loss: 0.0511 - val_acc: 0.9898\n",
      "Epoch 26/100\n",
      "60000/60000 [==============================] - 10s 168us/step - loss: 3.0866e-04 - acc: 1.0000 - val_loss: 0.0513 - val_acc: 0.9898\n",
      "Epoch 27/100\n",
      "60000/60000 [==============================] - 10s 165us/step - loss: 3.0658e-04 - acc: 1.0000 - val_loss: 0.0514 - val_acc: 0.9899\n",
      "Epoch 28/100\n",
      "60000/60000 [==============================] - 10s 169us/step - loss: 3.0481e-04 - acc: 1.0000 - val_loss: 0.0518 - val_acc: 0.9899\n",
      "Epoch 29/100\n",
      "60000/60000 [==============================] - 10s 166us/step - loss: 3.0317e-04 - acc: 1.0000 - val_loss: 0.0522 - val_acc: 0.9899\n",
      "Epoch 30/100\n",
      "60000/60000 [==============================] - 10s 170us/step - loss: 3.0158e-04 - acc: 1.0000 - val_loss: 0.0522 - val_acc: 0.9898\n",
      "Epoch 31/100\n",
      "60000/60000 [==============================] - 10s 167us/step - loss: 3.0024e-04 - acc: 1.0000 - val_loss: 0.0524 - val_acc: 0.9899\n",
      "Epoch 32/100\n",
      "60000/60000 [==============================] - 10s 169us/step - loss: 2.9895e-04 - acc: 1.0000 - val_loss: 0.0527 - val_acc: 0.9898\n",
      "Epoch 33/100\n",
      "60000/60000 [==============================] - 10s 167us/step - loss: 2.9777e-04 - acc: 1.0000 - val_loss: 0.0529 - val_acc: 0.9898\n",
      "Epoch 34/100\n",
      "60000/60000 [==============================] - 10s 168us/step - loss: 2.9667e-04 - acc: 1.0000 - val_loss: 0.0531 - val_acc: 0.9898\n",
      "Epoch 35/100\n",
      "60000/60000 [==============================] - 10s 167us/step - loss: 2.9558e-04 - acc: 1.0000 - val_loss: 0.0533 - val_acc: 0.9900\n",
      "Epoch 36/100\n",
      "60000/60000 [==============================] - 10s 170us/step - loss: 2.9471e-04 - acc: 1.0000 - val_loss: 0.0535 - val_acc: 0.9899\n",
      "Epoch 37/100\n",
      "60000/60000 [==============================] - 10s 167us/step - loss: 2.9376e-04 - acc: 1.0000 - val_loss: 0.0536 - val_acc: 0.9898\n",
      "Epoch 38/100\n",
      "60000/60000 [==============================] - 10s 168us/step - loss: 2.9293e-04 - acc: 1.0000 - val_loss: 0.0538 - val_acc: 0.9899\n",
      "Epoch 39/100\n",
      "60000/60000 [==============================] - 10s 168us/step - loss: 2.9216e-04 - acc: 1.0000 - val_loss: 0.0539 - val_acc: 0.9898\n",
      "Epoch 40/100\n",
      "60000/60000 [==============================] - 10s 169us/step - loss: 2.9139e-04 - acc: 1.0000 - val_loss: 0.0542 - val_acc: 0.9898\n",
      "Epoch 41/100\n",
      "60000/60000 [==============================] - 10s 167us/step - loss: 2.9071e-04 - acc: 1.0000 - val_loss: 0.0544 - val_acc: 0.9897\n",
      "Epoch 42/100\n",
      "60000/60000 [==============================] - 10s 169us/step - loss: 2.9007e-04 - acc: 1.0000 - val_loss: 0.0545 - val_acc: 0.9897\n",
      "Epoch 43/100\n",
      "60000/60000 [==============================] - 10s 168us/step - loss: 2.8944e-04 - acc: 1.0000 - val_loss: 0.0546 - val_acc: 0.9898\n",
      "Epoch 44/100\n",
      "60000/60000 [==============================] - 10s 168us/step - loss: 2.8884e-04 - acc: 1.0000 - val_loss: 0.0547 - val_acc: 0.9898\n",
      "Epoch 45/100\n",
      "60000/60000 [==============================] - 10s 167us/step - loss: 2.8831e-04 - acc: 1.0000 - val_loss: 0.0548 - val_acc: 0.9898\n",
      "Epoch 46/100\n",
      "60000/60000 [==============================] - 10s 171us/step - loss: 2.8775e-04 - acc: 1.0000 - val_loss: 0.0549 - val_acc: 0.9898\n",
      "Epoch 47/100\n",
      "60000/60000 [==============================] - 10s 169us/step - loss: 2.8728e-04 - acc: 1.0000 - val_loss: 0.0551 - val_acc: 0.9898\n",
      "Epoch 48/100\n",
      "60000/60000 [==============================] - 10s 170us/step - loss: 2.8679e-04 - acc: 1.0000 - val_loss: 0.0553 - val_acc: 0.9898\n",
      "Epoch 49/100\n",
      "60000/60000 [==============================] - 10s 168us/step - loss: 2.8636e-04 - acc: 1.0000 - val_loss: 0.0554 - val_acc: 0.9898\n",
      "Epoch 50/100\n",
      "60000/60000 [==============================] - 10s 169us/step - loss: 2.8591e-04 - acc: 1.0000 - val_loss: 0.0555 - val_acc: 0.9897\n",
      "Epoch 51/100\n",
      "60000/60000 [==============================] - 10s 168us/step - loss: 2.8548e-04 - acc: 1.0000 - val_loss: 0.0557 - val_acc: 0.9897\n",
      "Epoch 52/100\n",
      "60000/60000 [==============================] - 10s 169us/step - loss: 2.8508e-04 - acc: 1.0000 - val_loss: 0.0558 - val_acc: 0.9897\n",
      "Epoch 53/100\n",
      "60000/60000 [==============================] - 10s 167us/step - loss: 2.8472e-04 - acc: 1.0000 - val_loss: 0.0559 - val_acc: 0.9897\n",
      "Epoch 54/100\n",
      "60000/60000 [==============================] - 10s 169us/step - loss: 2.8436e-04 - acc: 1.0000 - val_loss: 0.0561 - val_acc: 0.9897\n",
      "Epoch 55/100\n",
      "60000/60000 [==============================] - 10s 167us/step - loss: 2.8400e-04 - acc: 1.0000 - val_loss: 0.0561 - val_acc: 0.9897\n",
      "Epoch 56/100\n",
      "60000/60000 [==============================] - 10s 169us/step - loss: 2.8368e-04 - acc: 1.0000 - val_loss: 0.0563 - val_acc: 0.9897\n",
      "Epoch 57/100\n",
      "60000/60000 [==============================] - 10s 167us/step - loss: 2.8336e-04 - acc: 1.0000 - val_loss: 0.0564 - val_acc: 0.9897\n",
      "Epoch 58/100\n",
      "60000/60000 [==============================] - 10s 169us/step - loss: 2.8305e-04 - acc: 1.0000 - val_loss: 0.0565 - val_acc: 0.9897\n",
      "Epoch 59/100\n",
      "60000/60000 [==============================] - 10s 167us/step - loss: 2.8276e-04 - acc: 1.0000 - val_loss: 0.0566 - val_acc: 0.9897\n",
      "Epoch 60/100\n",
      "60000/60000 [==============================] - 10s 169us/step - loss: 2.8248e-04 - acc: 1.0000 - val_loss: 0.0567 - val_acc: 0.9897\n",
      "Epoch 61/100\n",
      "60000/60000 [==============================] - 10s 167us/step - loss: 2.8221e-04 - acc: 1.0000 - val_loss: 0.0568 - val_acc: 0.9896\n",
      "Epoch 62/100\n",
      "60000/60000 [==============================] - 10s 169us/step - loss: 2.8194e-04 - acc: 1.0000 - val_loss: 0.0569 - val_acc: 0.9897\n",
      "Epoch 63/100\n",
      "60000/60000 [==============================] - 10s 166us/step - loss: 2.8168e-04 - acc: 1.0000 - val_loss: 0.0570 - val_acc: 0.9897\n",
      "Epoch 64/100\n",
      "60000/60000 [==============================] - 10s 169us/step - loss: 2.8144e-04 - acc: 1.0000 - val_loss: 0.0572 - val_acc: 0.9896\n",
      "Epoch 65/100\n",
      "60000/60000 [==============================] - 10s 168us/step - loss: 2.8120e-04 - acc: 1.0000 - val_loss: 0.0572 - val_acc: 0.9896\n",
      "Epoch 66/100\n",
      "60000/60000 [==============================] - 10s 169us/step - loss: 2.8098e-04 - acc: 1.0000 - val_loss: 0.0574 - val_acc: 0.9896\n",
      "Epoch 67/100\n",
      "60000/60000 [==============================] - 10s 168us/step - loss: 2.8076e-04 - acc: 1.0000 - val_loss: 0.0574 - val_acc: 0.9896\n",
      "Epoch 68/100\n",
      "60000/60000 [==============================] - 10s 169us/step - loss: 2.8055e-04 - acc: 1.0000 - val_loss: 0.0576 - val_acc: 0.9896\n",
      "Epoch 69/100\n",
      "60000/60000 [==============================] - 10s 167us/step - loss: 2.8035e-04 - acc: 1.0000 - val_loss: 0.0576 - val_acc: 0.9896\n",
      "Epoch 70/100\n",
      "60000/60000 [==============================] - 10s 168us/step - loss: 2.8014e-04 - acc: 1.0000 - val_loss: 0.0577 - val_acc: 0.9895\n",
      "Epoch 71/100\n",
      "60000/60000 [==============================] - 10s 168us/step - loss: 2.7995e-04 - acc: 1.0000 - val_loss: 0.0578 - val_acc: 0.9894\n",
      "Epoch 72/100\n",
      "60000/60000 [==============================] - 10s 168us/step - loss: 2.7976e-04 - acc: 1.0000 - val_loss: 0.0580 - val_acc: 0.9895\n",
      "Epoch 73/100\n",
      "60000/60000 [==============================] - 10s 168us/step - loss: 2.7958e-04 - acc: 1.0000 - val_loss: 0.0581 - val_acc: 0.9895\n",
      "Epoch 74/100\n",
      "60000/60000 [==============================] - 10s 169us/step - loss: 2.7941e-04 - acc: 1.0000 - val_loss: 0.0581 - val_acc: 0.9895\n",
      "Epoch 75/100\n",
      "60000/60000 [==============================] - 10s 168us/step - loss: 2.7924e-04 - acc: 1.0000 - val_loss: 0.0582 - val_acc: 0.9894\n",
      "Epoch 76/100\n",
      "60000/60000 [==============================] - 10s 169us/step - loss: 2.7907e-04 - acc: 1.0000 - val_loss: 0.0582 - val_acc: 0.9895\n",
      "Epoch 77/100\n",
      "60000/60000 [==============================] - 10s 169us/step - loss: 2.7891e-04 - acc: 1.0000 - val_loss: 0.0583 - val_acc: 0.9894\n",
      "Epoch 78/100\n",
      "60000/60000 [==============================] - 10s 169us/step - loss: 2.7876e-04 - acc: 1.0000 - val_loss: 0.0584 - val_acc: 0.9894\n",
      "Epoch 79/100\n",
      "60000/60000 [==============================] - 10s 168us/step - loss: 2.7860e-04 - acc: 1.0000 - val_loss: 0.0585 - val_acc: 0.9895\n",
      "Epoch 80/100\n",
      "60000/60000 [==============================] - 10s 169us/step - loss: 2.7846e-04 - acc: 1.0000 - val_loss: 0.0585 - val_acc: 0.9894\n",
      "Epoch 81/100\n",
      "60000/60000 [==============================] - 10s 167us/step - loss: 2.7831e-04 - acc: 1.0000 - val_loss: 0.0587 - val_acc: 0.9895\n",
      "Epoch 82/100\n",
      "60000/60000 [==============================] - 10s 167us/step - loss: 2.7817e-04 - acc: 1.0000 - val_loss: 0.0587 - val_acc: 0.9894\n",
      "Epoch 83/100\n",
      "60000/60000 [==============================] - 10s 168us/step - loss: 2.7804e-04 - acc: 1.0000 - val_loss: 0.0588 - val_acc: 0.9894\n",
      "Epoch 84/100\n",
      "60000/60000 [==============================] - 10s 167us/step - loss: 2.7791e-04 - acc: 1.0000 - val_loss: 0.0588 - val_acc: 0.9894\n",
      "Epoch 85/100\n",
      "60000/60000 [==============================] - 10s 168us/step - loss: 2.7776e-04 - acc: 1.0000 - val_loss: 0.0590 - val_acc: 0.9894\n",
      "Epoch 86/100\n",
      "60000/60000 [==============================] - 10s 167us/step - loss: 2.7766e-04 - acc: 1.0000 - val_loss: 0.0590 - val_acc: 0.9894\n",
      "Epoch 87/100\n",
      "60000/60000 [==============================] - 10s 169us/step - loss: 2.7753e-04 - acc: 1.0000 - val_loss: 0.0591 - val_acc: 0.9895\n",
      "Epoch 88/100\n",
      "60000/60000 [==============================] - 10s 168us/step - loss: 2.7741e-04 - acc: 1.0000 - val_loss: 0.0591 - val_acc: 0.9894\n",
      "Epoch 89/100\n",
      "60000/60000 [==============================] - 10s 169us/step - loss: 2.7730e-04 - acc: 1.0000 - val_loss: 0.0592 - val_acc: 0.9894\n",
      "Epoch 90/100\n",
      "60000/60000 [==============================] - 10s 167us/step - loss: 2.7718e-04 - acc: 1.0000 - val_loss: 0.0592 - val_acc: 0.9894\n",
      "Epoch 91/100\n",
      "60000/60000 [==============================] - 10s 168us/step - loss: 2.7707e-04 - acc: 1.0000 - val_loss: 0.0593 - val_acc: 0.9894\n",
      "Epoch 92/100\n",
      "60000/60000 [==============================] - 10s 167us/step - loss: 2.7697e-04 - acc: 1.0000 - val_loss: 0.0594 - val_acc: 0.9894\n",
      "Epoch 93/100\n",
      "60000/60000 [==============================] - 10s 168us/step - loss: 2.7686e-04 - acc: 1.0000 - val_loss: 0.0595 - val_acc: 0.9894\n",
      "Epoch 94/100\n",
      "60000/60000 [==============================] - 10s 167us/step - loss: 2.7676e-04 - acc: 1.0000 - val_loss: 0.0595 - val_acc: 0.9894\n",
      "Epoch 95/100\n",
      "60000/60000 [==============================] - 10s 169us/step - loss: 2.7666e-04 - acc: 1.0000 - val_loss: 0.0596 - val_acc: 0.9894\n",
      "Epoch 96/100\n",
      "60000/60000 [==============================] - 10s 167us/step - loss: 2.7656e-04 - acc: 1.0000 - val_loss: 0.0596 - val_acc: 0.9894\n",
      "Epoch 97/100\n",
      "60000/60000 [==============================] - 10s 169us/step - loss: 2.7647e-04 - acc: 1.0000 - val_loss: 0.0597 - val_acc: 0.9895\n",
      "Epoch 98/100\n",
      "60000/60000 [==============================] - 10s 167us/step - loss: 2.7637e-04 - acc: 1.0000 - val_loss: 0.0598 - val_acc: 0.9895\n",
      "Epoch 99/100\n",
      "60000/60000 [==============================] - 10s 169us/step - loss: 2.7629e-04 - acc: 1.0000 - val_loss: 0.0598 - val_acc: 0.9895\n",
      "Epoch 100/100\n",
      "60000/60000 [==============================] - 10s 166us/step - loss: 2.7619e-04 - acc: 1.0000 - val_loss: 0.0599 - val_acc: 0.9894\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fd1b8de7cf8>"
      ]
     },
     "execution_count": 51,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = create_model()\n",
    "model.compile(optimizer='sgd', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(x=images, y=labels, batch_size=64, epochs=100, validation_data=(test_images, test_labels))\n",
    "# loss は損失関数の値、acc は精度（正解率）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7BRFgLfS8gQF"
   },
   "source": [
    "acc（精度）が 1.000 になっています。\n",
    "これは、学習データすべてに対して完璧に正解を当てられているということです。\n",
    "\n",
    "一方で、val_acc（validation accuracy, 学習に使っていないデータにおける精度）は 0.99 程度が限界になっています。\n",
    "これが過学習という問題で、これを起こさないようにするのが deep learning の難しさの一つです。\n",
    "（データをもっとたくさん与えれば過学習問題は軽減できます。）\n",
    "\n",
    "\n",
    "また、↑にあげたモデル以外にもいろいろなモデル設計を考えることができます。\n",
    "以下にいくつかの例をあげます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 391
    },
    "colab_type": "code",
    "id": "uGYIRQNU6k-S",
    "outputId": "d9e1bfbd-c224-457f-b401-dbe856232fa0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 4s 62us/step - loss: 11.5588 - acc: 0.2824 - val_loss: 11.2469 - val_acc: 0.3019\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 4s 63us/step - loss: 9.9649 - acc: 0.3814 - val_loss: 9.8542 - val_acc: 0.3884\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 4s 62us/step - loss: 9.7936 - acc: 0.3921 - val_loss: 9.6658 - val_acc: 0.4000\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 4s 61us/step - loss: 9.7635 - acc: 0.3940 - val_loss: 9.8296 - val_acc: 0.3899\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 4s 62us/step - loss: 9.8152 - acc: 0.3909 - val_loss: 9.6607 - val_acc: 0.4006\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 4s 60us/step - loss: 9.7976 - acc: 0.3920 - val_loss: 9.9709 - val_acc: 0.3811\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 3s 57us/step - loss: 9.7531 - acc: 0.3948 - val_loss: 9.6584 - val_acc: 0.4005\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 4s 61us/step - loss: 9.7493 - acc: 0.3950 - val_loss: 9.7185 - val_acc: 0.3969\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 4s 61us/step - loss: 9.7082 - acc: 0.3975 - val_loss: 9.6188 - val_acc: 0.4032\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 4s 62us/step - loss: 9.7101 - acc: 0.3975 - val_loss: 9.6306 - val_acc: 0.4025\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fd1b8d47b38>"
      ]
     },
     "execution_count": 54,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ものすごく simple なモデル。精度は全然あがらない。\n",
    "def create_simple_model():\n",
    "  input = Input(shape=(28, 28, 1))\n",
    "  x = Flatten()(input)\n",
    "  x = Dense(10)(x)\n",
    "  output = Activation('softmax')(x)\n",
    "  return tf.keras.Model(input, output)\n",
    "\n",
    "model = create_simple_model()\n",
    "model.compile(optimizer='sgd', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(x=images, y=labels, batch_size=64, epochs=10, validation_data=(test_images, test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 3451
    },
    "colab_type": "code",
    "id": "nSkdX03W77cM",
    "outputId": "4974d8c1-abf4-436d-e051-d74357558e9d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/100\n",
      "60000/60000 [==============================] - 14s 238us/step - loss: 0.2576 - acc: 0.9198 - val_loss: 0.0758 - val_acc: 0.9767\n",
      "Epoch 2/100\n",
      "60000/60000 [==============================] - 13s 223us/step - loss: 0.1046 - acc: 0.9688 - val_loss: 0.0556 - val_acc: 0.9812\n",
      "Epoch 3/100\n",
      "60000/60000 [==============================] - 13s 223us/step - loss: 0.0785 - acc: 0.9761 - val_loss: 0.0473 - val_acc: 0.9846\n",
      "Epoch 4/100\n",
      "60000/60000 [==============================] - 13s 223us/step - loss: 0.0651 - acc: 0.9805 - val_loss: 0.0431 - val_acc: 0.9861\n",
      "Epoch 5/100\n",
      "60000/60000 [==============================] - 13s 220us/step - loss: 0.0567 - acc: 0.9824 - val_loss: 0.0352 - val_acc: 0.9889\n",
      "Epoch 6/100\n",
      "60000/60000 [==============================] - 13s 222us/step - loss: 0.0497 - acc: 0.9842 - val_loss: 0.0345 - val_acc: 0.9883\n",
      "Epoch 7/100\n",
      "60000/60000 [==============================] - 13s 220us/step - loss: 0.0452 - acc: 0.9860 - val_loss: 0.0345 - val_acc: 0.9877\n",
      "Epoch 8/100\n",
      "60000/60000 [==============================] - 13s 223us/step - loss: 0.0404 - acc: 0.9874 - val_loss: 0.0338 - val_acc: 0.9881\n",
      "Epoch 9/100\n",
      "60000/60000 [==============================] - 13s 223us/step - loss: 0.0379 - acc: 0.9884 - val_loss: 0.0301 - val_acc: 0.9905\n",
      "Epoch 10/100\n",
      "60000/60000 [==============================] - 13s 221us/step - loss: 0.0344 - acc: 0.9894 - val_loss: 0.0315 - val_acc: 0.9900\n",
      "Epoch 11/100\n",
      "60000/60000 [==============================] - 13s 222us/step - loss: 0.0315 - acc: 0.9904 - val_loss: 0.0291 - val_acc: 0.9901\n",
      "Epoch 12/100\n",
      "60000/60000 [==============================] - 13s 221us/step - loss: 0.0295 - acc: 0.9906 - val_loss: 0.0289 - val_acc: 0.9903\n",
      "Epoch 13/100\n",
      "60000/60000 [==============================] - 13s 224us/step - loss: 0.0278 - acc: 0.9912 - val_loss: 0.0288 - val_acc: 0.9897\n",
      "Epoch 14/100\n",
      "60000/60000 [==============================] - 13s 222us/step - loss: 0.0269 - acc: 0.9917 - val_loss: 0.0253 - val_acc: 0.9914\n",
      "Epoch 15/100\n",
      "60000/60000 [==============================] - 13s 224us/step - loss: 0.0258 - acc: 0.9912 - val_loss: 0.0292 - val_acc: 0.9899\n",
      "Epoch 16/100\n",
      "60000/60000 [==============================] - 13s 221us/step - loss: 0.0233 - acc: 0.9927 - val_loss: 0.0278 - val_acc: 0.9911\n",
      "Epoch 17/100\n",
      "60000/60000 [==============================] - 13s 222us/step - loss: 0.0223 - acc: 0.9928 - val_loss: 0.0254 - val_acc: 0.9917\n",
      "Epoch 18/100\n",
      "60000/60000 [==============================] - 13s 222us/step - loss: 0.0218 - acc: 0.9927 - val_loss: 0.0261 - val_acc: 0.9915\n",
      "Epoch 19/100\n",
      "60000/60000 [==============================] - 13s 220us/step - loss: 0.0202 - acc: 0.9932 - val_loss: 0.0262 - val_acc: 0.9913\n",
      "Epoch 20/100\n",
      "60000/60000 [==============================] - 13s 222us/step - loss: 0.0198 - acc: 0.9940 - val_loss: 0.0240 - val_acc: 0.9919\n",
      "Epoch 21/100\n",
      "60000/60000 [==============================] - 13s 222us/step - loss: 0.0185 - acc: 0.9936 - val_loss: 0.0258 - val_acc: 0.9910\n",
      "Epoch 22/100\n",
      "60000/60000 [==============================] - 13s 220us/step - loss: 0.0182 - acc: 0.9942 - val_loss: 0.0264 - val_acc: 0.9911\n",
      "Epoch 23/100\n",
      "60000/60000 [==============================] - 13s 222us/step - loss: 0.0164 - acc: 0.9948 - val_loss: 0.0243 - val_acc: 0.9919\n",
      "Epoch 24/100\n",
      "60000/60000 [==============================] - 13s 223us/step - loss: 0.0172 - acc: 0.9944 - val_loss: 0.0323 - val_acc: 0.9901\n",
      "Epoch 25/100\n",
      "60000/60000 [==============================] - 13s 221us/step - loss: 0.0160 - acc: 0.9949 - val_loss: 0.0251 - val_acc: 0.9923\n",
      "Epoch 26/100\n",
      "60000/60000 [==============================] - 13s 225us/step - loss: 0.0153 - acc: 0.9949 - val_loss: 0.0263 - val_acc: 0.9914\n",
      "Epoch 27/100\n",
      "60000/60000 [==============================] - 13s 223us/step - loss: 0.0146 - acc: 0.9951 - val_loss: 0.0250 - val_acc: 0.9921\n",
      "Epoch 28/100\n",
      "60000/60000 [==============================] - 13s 221us/step - loss: 0.0138 - acc: 0.9956 - val_loss: 0.0287 - val_acc: 0.9908\n",
      "Epoch 29/100\n",
      "60000/60000 [==============================] - 13s 222us/step - loss: 0.0130 - acc: 0.9957 - val_loss: 0.0245 - val_acc: 0.9928\n",
      "Epoch 30/100\n",
      "60000/60000 [==============================] - 13s 221us/step - loss: 0.0123 - acc: 0.9963 - val_loss: 0.0279 - val_acc: 0.9909\n",
      "Epoch 31/100\n",
      "60000/60000 [==============================] - 13s 220us/step - loss: 0.0123 - acc: 0.9961 - val_loss: 0.0252 - val_acc: 0.9918\n",
      "Epoch 32/100\n",
      "60000/60000 [==============================] - 13s 222us/step - loss: 0.0118 - acc: 0.9961 - val_loss: 0.0255 - val_acc: 0.9918\n",
      "Epoch 33/100\n",
      "60000/60000 [==============================] - 13s 221us/step - loss: 0.0117 - acc: 0.9964 - val_loss: 0.0252 - val_acc: 0.9917\n",
      "Epoch 34/100\n",
      "60000/60000 [==============================] - 13s 221us/step - loss: 0.0107 - acc: 0.9969 - val_loss: 0.0259 - val_acc: 0.9920\n",
      "Epoch 35/100\n",
      "60000/60000 [==============================] - 13s 221us/step - loss: 0.0116 - acc: 0.9963 - val_loss: 0.0265 - val_acc: 0.9923\n",
      "Epoch 36/100\n",
      "60000/60000 [==============================] - 13s 220us/step - loss: 0.0112 - acc: 0.9964 - val_loss: 0.0258 - val_acc: 0.9917\n",
      "Epoch 37/100\n",
      "60000/60000 [==============================] - 13s 222us/step - loss: 0.0107 - acc: 0.9969 - val_loss: 0.0238 - val_acc: 0.9920\n",
      "Epoch 38/100\n",
      "60000/60000 [==============================] - 13s 223us/step - loss: 0.0100 - acc: 0.9970 - val_loss: 0.0242 - val_acc: 0.9927\n",
      "Epoch 39/100\n",
      "60000/60000 [==============================] - 15s 243us/step - loss: 0.0097 - acc: 0.9970 - val_loss: 0.0258 - val_acc: 0.9921\n",
      "Epoch 40/100\n",
      "60000/60000 [==============================] - 14s 229us/step - loss: 0.0096 - acc: 0.9970 - val_loss: 0.0249 - val_acc: 0.9922\n",
      "Epoch 41/100\n",
      "60000/60000 [==============================] - 13s 225us/step - loss: 0.0097 - acc: 0.9970 - val_loss: 0.0260 - val_acc: 0.9917\n",
      "Epoch 42/100\n",
      "60000/60000 [==============================] - 14s 226us/step - loss: 0.0094 - acc: 0.9971 - val_loss: 0.0254 - val_acc: 0.9920\n",
      "Epoch 43/100\n",
      "60000/60000 [==============================] - 14s 226us/step - loss: 0.0090 - acc: 0.9972 - val_loss: 0.0262 - val_acc: 0.9923\n",
      "Epoch 44/100\n",
      "60000/60000 [==============================] - 14s 226us/step - loss: 0.0083 - acc: 0.9974 - val_loss: 0.0257 - val_acc: 0.9923\n",
      "Epoch 45/100\n",
      "60000/60000 [==============================] - 14s 227us/step - loss: 0.0082 - acc: 0.9973 - val_loss: 0.0246 - val_acc: 0.9928\n",
      "Epoch 46/100\n",
      "60000/60000 [==============================] - 14s 228us/step - loss: 0.0083 - acc: 0.9975 - val_loss: 0.0259 - val_acc: 0.9924\n",
      "Epoch 47/100\n",
      "60000/60000 [==============================] - 14s 229us/step - loss: 0.0087 - acc: 0.9973 - val_loss: 0.0240 - val_acc: 0.9926\n",
      "Epoch 48/100\n",
      "60000/60000 [==============================] - 14s 227us/step - loss: 0.0069 - acc: 0.9980 - val_loss: 0.0277 - val_acc: 0.9923\n",
      "Epoch 49/100\n",
      "60000/60000 [==============================] - 14s 227us/step - loss: 0.0077 - acc: 0.9975 - val_loss: 0.0288 - val_acc: 0.9919\n",
      "Epoch 50/100\n",
      "60000/60000 [==============================] - 14s 228us/step - loss: 0.0079 - acc: 0.9974 - val_loss: 0.0234 - val_acc: 0.9930\n",
      "Epoch 51/100\n",
      "60000/60000 [==============================] - 14s 225us/step - loss: 0.0073 - acc: 0.9978 - val_loss: 0.0237 - val_acc: 0.9927\n",
      "Epoch 52/100\n",
      "60000/60000 [==============================] - 14s 226us/step - loss: 0.0074 - acc: 0.9977 - val_loss: 0.0257 - val_acc: 0.9927\n",
      "Epoch 53/100\n",
      "60000/60000 [==============================] - 14s 225us/step - loss: 0.0073 - acc: 0.9977 - val_loss: 0.0240 - val_acc: 0.9932\n",
      "Epoch 54/100\n",
      "60000/60000 [==============================] - 13s 224us/step - loss: 0.0069 - acc: 0.9978 - val_loss: 0.0304 - val_acc: 0.9914\n",
      "Epoch 55/100\n",
      "60000/60000 [==============================] - 14s 226us/step - loss: 0.0067 - acc: 0.9979 - val_loss: 0.0256 - val_acc: 0.9928\n",
      "Epoch 56/100\n",
      "60000/60000 [==============================] - 14s 228us/step - loss: 0.0066 - acc: 0.9981 - val_loss: 0.0243 - val_acc: 0.9928\n",
      "Epoch 57/100\n",
      "60000/60000 [==============================] - 13s 224us/step - loss: 0.0068 - acc: 0.9979 - val_loss: 0.0277 - val_acc: 0.9924\n",
      "Epoch 58/100\n",
      "60000/60000 [==============================] - 13s 225us/step - loss: 0.0061 - acc: 0.9980 - val_loss: 0.0273 - val_acc: 0.9926\n",
      "Epoch 59/100\n",
      "60000/60000 [==============================] - 13s 225us/step - loss: 0.0060 - acc: 0.9980 - val_loss: 0.0252 - val_acc: 0.9929\n",
      "Epoch 60/100\n",
      "60000/60000 [==============================] - 13s 224us/step - loss: 0.0055 - acc: 0.9983 - val_loss: 0.0249 - val_acc: 0.9928\n",
      "Epoch 61/100\n",
      "60000/60000 [==============================] - 14s 226us/step - loss: 0.0059 - acc: 0.9982 - val_loss: 0.0283 - val_acc: 0.9923\n",
      "Epoch 62/100\n",
      "60000/60000 [==============================] - 14s 226us/step - loss: 0.0058 - acc: 0.9980 - val_loss: 0.0256 - val_acc: 0.9928\n",
      "Epoch 63/100\n",
      "60000/60000 [==============================] - 13s 225us/step - loss: 0.0055 - acc: 0.9985 - val_loss: 0.0263 - val_acc: 0.9925\n",
      "Epoch 64/100\n",
      "60000/60000 [==============================] - 14s 226us/step - loss: 0.0054 - acc: 0.9983 - val_loss: 0.0269 - val_acc: 0.9926\n",
      "Epoch 65/100\n",
      "60000/60000 [==============================] - 14s 227us/step - loss: 0.0053 - acc: 0.9983 - val_loss: 0.0250 - val_acc: 0.9931\n",
      "Epoch 66/100\n",
      "60000/60000 [==============================] - 13s 224us/step - loss: 0.0054 - acc: 0.9983 - val_loss: 0.0252 - val_acc: 0.9933\n",
      "Epoch 67/100\n",
      "60000/60000 [==============================] - 14s 228us/step - loss: 0.0060 - acc: 0.9980 - val_loss: 0.0272 - val_acc: 0.9923\n",
      "Epoch 68/100\n",
      "60000/60000 [==============================] - 14s 225us/step - loss: 0.0051 - acc: 0.9983 - val_loss: 0.0256 - val_acc: 0.9932\n",
      "Epoch 69/100\n",
      "60000/60000 [==============================] - 13s 225us/step - loss: 0.0046 - acc: 0.9986 - val_loss: 0.0258 - val_acc: 0.9929\n",
      "Epoch 70/100\n",
      "60000/60000 [==============================] - 14s 226us/step - loss: 0.0047 - acc: 0.9986 - val_loss: 0.0248 - val_acc: 0.9929\n",
      "Epoch 71/100\n",
      "60000/60000 [==============================] - 14s 227us/step - loss: 0.0051 - acc: 0.9985 - val_loss: 0.0293 - val_acc: 0.9916\n",
      "Epoch 72/100\n",
      "60000/60000 [==============================] - 13s 224us/step - loss: 0.0048 - acc: 0.9985 - val_loss: 0.0265 - val_acc: 0.9929\n",
      "Epoch 73/100\n",
      "60000/60000 [==============================] - 14s 226us/step - loss: 0.0050 - acc: 0.9984 - val_loss: 0.0267 - val_acc: 0.9925\n",
      "Epoch 74/100\n",
      "60000/60000 [==============================] - 14s 228us/step - loss: 0.0048 - acc: 0.9983 - val_loss: 0.0260 - val_acc: 0.9928\n",
      "Epoch 75/100\n",
      "60000/60000 [==============================] - 14s 227us/step - loss: 0.0049 - acc: 0.9983 - val_loss: 0.0284 - val_acc: 0.9922\n",
      "Epoch 76/100\n",
      "60000/60000 [==============================] - 14s 225us/step - loss: 0.0041 - acc: 0.9988 - val_loss: 0.0260 - val_acc: 0.9923\n",
      "Epoch 77/100\n",
      "60000/60000 [==============================] - 14s 226us/step - loss: 0.0048 - acc: 0.9985 - val_loss: 0.0264 - val_acc: 0.9929\n",
      "Epoch 78/100\n",
      "60000/60000 [==============================] - 13s 224us/step - loss: 0.0041 - acc: 0.9987 - val_loss: 0.0255 - val_acc: 0.9930\n",
      "Epoch 79/100\n",
      "60000/60000 [==============================] - 13s 225us/step - loss: 0.0046 - acc: 0.9986 - val_loss: 0.0286 - val_acc: 0.9925\n",
      "Epoch 80/100\n",
      "60000/60000 [==============================] - 13s 224us/step - loss: 0.0046 - acc: 0.9985 - val_loss: 0.0263 - val_acc: 0.9922\n",
      "Epoch 81/100\n",
      "60000/60000 [==============================] - 13s 225us/step - loss: 0.0040 - acc: 0.9989 - val_loss: 0.0271 - val_acc: 0.9925\n",
      "Epoch 82/100\n",
      "60000/60000 [==============================] - 14s 226us/step - loss: 0.0044 - acc: 0.9987 - val_loss: 0.0265 - val_acc: 0.9930\n",
      "Epoch 83/100\n",
      "60000/60000 [==============================] - 13s 225us/step - loss: 0.0040 - acc: 0.9989 - val_loss: 0.0281 - val_acc: 0.9930\n",
      "Epoch 84/100\n",
      "60000/60000 [==============================] - 14s 225us/step - loss: 0.0044 - acc: 0.9985 - val_loss: 0.0260 - val_acc: 0.9928\n",
      "Epoch 85/100\n",
      "60000/60000 [==============================] - 14s 225us/step - loss: 0.0038 - acc: 0.9988 - val_loss: 0.0273 - val_acc: 0.9926\n",
      "Epoch 86/100\n",
      "60000/60000 [==============================] - 14s 226us/step - loss: 0.0037 - acc: 0.9990 - val_loss: 0.0266 - val_acc: 0.9930\n",
      "Epoch 87/100\n",
      "60000/60000 [==============================] - 14s 227us/step - loss: 0.0040 - acc: 0.9988 - val_loss: 0.0264 - val_acc: 0.9927\n",
      "Epoch 88/100\n",
      "60000/60000 [==============================] - 14s 227us/step - loss: 0.0042 - acc: 0.9987 - val_loss: 0.0270 - val_acc: 0.9924\n",
      "Epoch 89/100\n",
      "60000/60000 [==============================] - 14s 227us/step - loss: 0.0044 - acc: 0.9988 - val_loss: 0.0257 - val_acc: 0.9935\n",
      "Epoch 90/100\n",
      "60000/60000 [==============================] - 13s 224us/step - loss: 0.0038 - acc: 0.9987 - val_loss: 0.0269 - val_acc: 0.9923\n",
      "Epoch 91/100\n",
      "60000/60000 [==============================] - 14s 226us/step - loss: 0.0038 - acc: 0.9987 - val_loss: 0.0292 - val_acc: 0.9926\n",
      "Epoch 92/100\n",
      "60000/60000 [==============================] - 14s 226us/step - loss: 0.0043 - acc: 0.9987 - val_loss: 0.0277 - val_acc: 0.9925\n",
      "Epoch 93/100\n",
      "60000/60000 [==============================] - 13s 225us/step - loss: 0.0035 - acc: 0.9988 - val_loss: 0.0258 - val_acc: 0.9933\n",
      "Epoch 94/100\n",
      "60000/60000 [==============================] - 14s 225us/step - loss: 0.0035 - acc: 0.9990 - val_loss: 0.0261 - val_acc: 0.9930\n",
      "Epoch 95/100\n",
      "60000/60000 [==============================] - 13s 225us/step - loss: 0.0035 - acc: 0.9990 - val_loss: 0.0274 - val_acc: 0.9931\n",
      "Epoch 96/100\n",
      "60000/60000 [==============================] - 14s 225us/step - loss: 0.0032 - acc: 0.9992 - val_loss: 0.0258 - val_acc: 0.9930\n",
      "Epoch 97/100\n",
      "60000/60000 [==============================] - 14s 228us/step - loss: 0.0034 - acc: 0.9991 - val_loss: 0.0257 - val_acc: 0.9930\n",
      "Epoch 98/100\n",
      "60000/60000 [==============================] - 14s 227us/step - loss: 0.0034 - acc: 0.9990 - val_loss: 0.0260 - val_acc: 0.9930\n",
      "Epoch 99/100\n",
      "60000/60000 [==============================] - 14s 227us/step - loss: 0.0033 - acc: 0.9991 - val_loss: 0.0269 - val_acc: 0.9931\n",
      "Epoch 100/100\n",
      "60000/60000 [==============================] - 14s 226us/step - loss: 0.0040 - acc: 0.9987 - val_loss: 0.0251 - val_acc: 0.9939\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fd1b6659ba8>"
      ]
     },
     "execution_count": 58,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ちょっとだけ良いモデル。 val_acc が高くなっている。\n",
    "def create_better_model():\n",
    "    from tensorflow.python.keras.layers import BatchNormalization, Dropout\n",
    "    input = Input(shape=(28, 28, 1))\n",
    "    x = Conv2D(32, kernel_size=3)(input)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Conv2D(64, kernel_size=3)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = MaxPooling2D(pool_size=2)(x)\n",
    "    x = Flatten()(x)\n",
    "    x = Dropout(0.25)(x)\n",
    "    x = Dense(128)(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Dropout(0.25)(x)\n",
    "    x = Dense(10)(x)\n",
    "    output = Activation('softmax')(x)\n",
    "\n",
    "    return tf.keras.Model(input, output)\n",
    "\n",
    "model = create_better_model()\n",
    "model.compile(optimizer='sgd', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(x=images, y=labels, batch_size=64, epochs=100, validation_data=(test_images, test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Bqvj0LbTCymK"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "keras_intro_mnist.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

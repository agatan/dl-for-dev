{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "_ntSIdz_Szv9",
    "outputId": "b504269e-b15c-4170-ddd8-578b34bb050d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/sh: 1: nvidia-smi: not found\r\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "# このセルを実行して環境のチェックをしてください。\n",
    "\n",
    "!nvidia-smi\n",
    "# nvidia-smi: not found  になる場合は、ランタイム→ランタイムのタイプを変更→ハードウェアアクセラレータを GPU に変更してください。\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# あとでつかうデータセットをダウンロードしておきます。\n",
    "tf.keras.datasets.fashion_mnist.load_data()\n",
    "tf.keras.datasets.mnist.load_data() \n",
    "print('OK')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YFXZikTHUgM8"
   },
   "source": [
    "\n",
    "## 機械学習と deep learning のイメージを掴む\n",
    "\n",
    "典型的な機械学習とは、以下のようなプロセスを繰り返すものです。\n",
    "\n",
    "```python\n",
    "f = initialize_function()\n",
    "for _ in range(1000):\n",
    "  y = f(x)\n",
    "  l = loss(y)\n",
    "  f = update_function_to_minimize_loss(f, l)\n",
    "```\n",
    "\n",
    "ゴールは「入力データから（なんらかの基準に基づいて）最適な出力を得られるような関数」を得ることです。\n",
    "\n",
    "1. 適当な関数を定義する (例えば `f(x) = a * x ^ 2 + b * x + c, a = 1.2, b = 0.3, c = -1.4`)\n",
    "  - 必ずしも数学的な式で表せる必要はありません。たとえば if 文の塊で表現する場合もあります。（RandomForest）\n",
    "2. 学習のためのデータをその関数に入れ、何らかの出力を得る (`y = f(x)`)\n",
    "3. その出力がどのくらい良くなかったかを計算する (= 損失)\n",
    "  - 「猫の画像」に対して「猫である確率 0.2 」と予測したら損失は大きくなる\n",
    "4. 損失が小さくなるように、適当に作った関数のパラメータをいじる\n",
    "  - 例えば `a = 1.1, b = 0.4, c = -1.1` にする\n",
    "  - ここで微分が登場します（損失を a, b, c で偏微分すると a, b, c をどっちに動かせば損失が小さくなるかわかる）\n",
    "  \n",
    "\n",
    "教師あり学習の場合は、関数によって予測された `y` が正解データ `y_true` にどれだけ近いかが損失になります。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Tg1UN6B7Ueaf"
   },
   "source": [
    "## パーセプトロン\n",
    "\n",
    "deep learning とは深く層を重ねたニューラルネットワークを用いた学習のことです。\n",
    "ニューラルネットワークとは、たとえば以下のようなものから構成されます。\n",
    "\n",
    "deep learning のもっとも単純な構成要素は以下のような式でイメージできます。（これがニューラルネットワークの一層だと考えてください。）\n",
    "\n",
    "```python\n",
    "f(x) = w * x + b\n",
    "```\n",
    "\n",
    "ここで登場する `w, b` を↑で説明したようなステップで最適化することで、良い予測モデルをつくることができます。\n",
    "一方で、この単純な関数では表現力が弱すぎるため、データが複雑な関係性を持っている場合には良い予測ができなくなります。\n",
    "（たとえば、真の分布を表現する関数が `x ^ 2` だった場合を考えます。どう `w` や `b` を定義しても `w * x + b` では `x ^ 2` は表せません。）\n",
    "\n",
    "そこで、もっと層を重ねることで表現力を増すという方法を考えます。\n",
    "単純にやると以下のようになります。\n",
    "\n",
    "```python\n",
    "f(x) = w_2 * (w_1 * x + b_1) + b_2\n",
    "```\n",
    "\n",
    "実はこのままでは表現力は増してくれません。\n",
    "上の式を整理すると\n",
    "\n",
    "```\n",
    "f(x) = w'x + b'\n",
    "(ただし、w' = w_2 * w_1, b' = w_2 * b_1 + b2)\n",
    "```\n",
    "\n",
    "と表せてしまいます。つまり `f(x) = w * x + b` と同じ表現力になってしまうことがわかります。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ktb7y8Bd9cgv"
   },
   "source": [
    "## 活性化関数 / 非線形\n",
    "\n",
    "そこで登場するのが **活性化関数** です。\n",
    "簡単に言うと、層と層の間に挟む、ちょっと特殊な関数のことです。（※ 非線形な関数である必要があります。）\n",
    "活性化関数を `h` という名前で呼ぶとすると、\n",
    "\n",
    "```\n",
    "f(x) = w_2 * h(w_1 * x + b_1) + b_2\n",
    "```\n",
    "\n",
    "となります。式変形しても `f(x) = w * x + b` と表せなくなれば、一層で表される関数より複雑な表現力を持つ関数を作れたことになります。"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "abst.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
